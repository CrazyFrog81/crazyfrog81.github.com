<!doctype html>
<html>
<head>
    <meta name="Robots" contect= "none">
    <meta charset="UTF-8">

    <title>Zeng Wei's Homepage</title>
    <link rel="stylesheet" type="text/css" href="../zengwei.css" media="all" />
    <script src="../info.js" type="text/javascript" defer></script>
</head>

<body>

<div class="content">
    <div id="info" class="id">
    </div>

    <div class="main">
        <nav id="nav">
            <a href="..">HOME</a> &nbsp;&nbsp;
            <a href="../Research">RESEARCH</a> &nbsp;&nbsp;
            <a style="font-weight:bold">PUBLICATIONS</a> &nbsp;&nbsp;
            <a href="http://www.hkust-cival.com">TEAM</a> &nbsp;&nbsp;
            <a href="../vita">VITA</a> &nbsp;&nbsp;
<!--            <a href="../blog">BLOG</a> &nbsp;&nbsp;-->
            <a href="../bookmark">BOOKMARK</a> &nbsp;&nbsp;
        </nav>

        <h3 style="margin:0 0 5px 0">Featured Publications (<a href="https://scholar.google.com/citations?user=kTZhR2EAAAAJ&hl=en">All Publications</a>)</h3>
        <text>* Corresponding author, <u>Students/RAs</u> under my supervision</text> <br><br>

        <table>
            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2025%20AKRMap.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings</b><BR>
                    <u>Yilin Ye</u>, Junchao Huang, <u>Xingchen Zeng</u>, Jiazhi Xia, <strong>Wei Zeng*</strong><BR>
                    <em>Proceedings of International Conference on Machine Learning (ICML'25)</em>, 2025.<BR>
                    [<a href="">Homepage</a>]
                    [<a href="">arxiv</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2025_vizta.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>VizTA: Enhancing Comprehension of Distributional Visualization with Visual-Lexical Fused Conversational Interface</b><BR>
                    <u>Liangwei Wang</u>, <u>Zhan Wang</u>, <u>Shishi Xiao</u>, Le Liu, Fugee Tsung, <strong>Wei Zeng*</strong><BR>
                    <em>Computer Graphics Forum (Proc. EuroVis'25) </em>, 2025. Accepted.<BR>
                    [<a href="">Homepage</a>]
                    [<a href="https://arxiv.org/abs/2504.14507">arxiv</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2025_picasso.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Unified Visual Comparison Framework for Human and AI Paintings using Neural Embeddings and Computational Aesthetics</b><BR>
                    <u>Yilin Ye</u>, <u>Rong Huang</u>, Kang Zhang, <strong>Wei Zeng*</strong><BR>
                    <em>IEEE Computer Graphics and Applications (Special Issue on Generative AI for Computer Graphics) </em>, 2025. Accepted.<BR>
                    [<a href="">Homepage</a>]
                    [<a href="">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2025_gencolor.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>GenColor: Generative Color-Concept Association in Visual Design</b><BR>
                    <u>Yihan Hou</u>, <u>Xingchen Zeng</u>, <u>Yusong Wang</u>, <u>Manling Yang</u>, Xiaojiao Chen, <strong>Wei Zeng*</strong><BR>
                    <em>Proceedings of ACM CHI Conference on Human Factors in Computing Systems</em>, 2025. Accepted.<BR>
                    [<a href="">Homepage</a>]
                    [<a href="https://arxiv.org/abs/2503.03236">arxiv</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2025_sketchflex.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>SketchFlex: Facilitating Spatial-Semantic Coherence in Text-to-Image Generation with Region-Based Sketches</b><BR>
                    <u>Haichuan Lin</u>, <u>Yilin Ye</u>*, Jiazhi Xia, <strong>Wei Zeng</strong><BR>
                    <em>Proceedings of ACM CHI Conference on Human Factors in Computing Systems</em>, 2025. Accepted.
                    <span style="color: red">Best Paper Honourable Mention Award </span>
                    <img src="best_icon.png" alt="Icon" width="9" height="16" style="vertical-align: middle;"> <BR>
                    [<a href="">Homepage</a>]
                    [<a href="https://arxiv.org/abs/2502.07556">arxiv</a>]
                    [<a href="">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_finflier.jpg">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>FinFlier: Automating Graphical Overlays for Financial Visualizations with Knowledge-Grounding Large Language Model</b><BR>
                    <u>Jianing Hao</u>, <u>Manling Yang</u>, <u>Qing Shi</u>, <u>Yuzhe Jiang</u>, Guang Zhang, <strong>Wei Zeng*</strong><BR>
                    <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2025. Accepted.<BR>
                    [<a href="https://arxiv.org/abs/2412.06821v1">arxiv</a>]
                    [<a href="https://doi.org/10.1109/TVCG.2024.3514138">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024%20ICH.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Centennial Drama Reimagined: An Immersive Experience of Intangible Cultural Heritage through Contextual Storytelling in Virtual Reality</b><BR>
                    <u>Jian Yu</u>, <u>Zhan Wang</u>, <u>Yifan Cao</u>, <u>Hao Cui</u>, <strong>Wei Zeng*</strong><BR>
                    <em>ACM Journal on Computing and Cultural Heritage</em>, 2025, 18(1), Article No.: 11, Pages 1 - 22.<BR>
                    [<a href="https://dl.acm.org/doi/10.1145/3705613">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_AIRays.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>AI-rays: Exploring Bias in the Gaze of AI Through a Multimodal Interactive Installation</b><BR>
                    <u>Ziyao Gao</u>, <u>Yiwen Zhang</u>, <u>Ling Li</u>, Theodoros Papatheodorou, <strong>Wei Zeng*</strong><BR>
                    <em>Proceedings of Siggraph Asia 2024 Art Paper</em>, 2024, Article 1, 1 - 7.<BR>
                    [<a href="https://arxiv.org/abs/2410.03786">arxiv</a>]
                    [<a href="https://dl.acm.org/doi/10.1145/3680530.3695433">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_CQA.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning</b><BR>
                    <u>Xingchen Zeng</u>, <u>Haichuan Lin</u>, <u>Yilin Ye</u>, <strong>Wei Zeng*</strong><BR>
                    <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE VIS 2024)</em>, 31(1): 525 - 535, 2025.<BR>
                    [<a href="https://arxiv.org/abs/2407.20174">arxiv</a>]
                    [<a href="https://doi.org/10.1109/TVCG.2024.3456159">DOI</a>]
                    [<a href="https://github.com/zengxingchen/ChartQA-MLLM">Data & Code</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_modalchorus.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via Modal Fusion Map</b><BR>
                    <u>Yilin Ye</u>, <u>Shishi Xiao</u>, <u>Xingchen Zeng</u>, <strong>Wei Zeng*</strong><BR>
                    <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE VIS 2024)</em>, 31(1): 294 - 304, 2025.<BR>
                    [<a href="https://arxiv.org/abs/2407.12315">arxiv</a>]
                    [<a href="https://doi.org/10.1109/TVCG.2024.3456387">DOI</a>]
                    [<a href="https://github.com/yilinye/modal-fusion-map">Data & Code</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_georeasoner.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>GeoReasoner: Geo-localization with Reasoning in Street Views using a Large Vision-Language Model</b><BR>
                    <u>Ling Li</u>, Yu Ye, Binchuan Jiang*, <strong>Wei Zeng</strong><BR>
                    <em>Proceedings of The International Conference on Machine Learning (ICML'24)</em>, Pages 29222-29233, 2024. <BR>
                    [<a href="https://arxiv.org/abs/2406.18572v1">arxiv</a>]
                    [<a href="https://github.com/lingli1996/GeoReasoner">Data & Code</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_GenAIVis.jpg">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Generative AI for Visualization: State of the Art and Future Directions</b><BR>
                    <u>Yilin Ye</u>, <u>Jianing Hao</u>, <u>Yihan Hou</u>, <u>Zhan Wang</u>, <u>Shishi Xiao</u>, Yuyu Luo, <strong>Wei Zeng*</strong><BR>
                    <em>Visual Informatics</em>, 8(2): 43-66, 2024. <BR>
                    [<a href="https://arxiv.org/abs/2404.18144">arxiv</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_VirtuWander.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>VirtuWander: Enhancing Multi-modal Interaction for Virtual Tour Guidance through Large Language Models</b><BR>
                    <u>Zhan Wang</u>, <u>Linping Yuan</u>, <u>Liangwei Wang</u>, Bingchuan Jiang, <strong>Wei Zeng*</strong><BR>
                    <em>Proceedings of The ACM CHI Conference on Human Factors in Computing Systems</em>, 2024, Article 612, 1–20.<BR>
                    [<a href="https://hkust-cival.com/projects/virtuwander/">Homepage</a>]
                    [<a href="https://arxiv.org/abs/2401.11923">arxiv</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
            <td>
                <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_C2ldeas.png">
            </td>
            <td style="padding-left: 10px; vertical-align: top">
                <b>C2Ideas: Supporting Creative Interior Color Design Ideation with Large Language Model</b><BR>
                <u>Yihan Hou</u>, <u>Manling Yang</u>, <u>Hao Cui</u>, Lei Wang, Jie Xu, <strong>Wei Zeng*</strong><BR>
                <em>Proceedings of The ACM CHI Conference on Human Factors in Computing Systems</em>, 2024, Article 172, 1–18.<BR>
                [<a href="https://arxiv.org/abs/2401.12586">arxiv</a>]
            </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_IntentTuner.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>IntentTuner: An Interactive Framework for Integrating Human Intentions in Fine-tuning Text-to-Image Generative Models</b><BR>
                    <u>Xingchen Zeng</u>, <u>Ziyao Gao</u>, <u>Yilin Ye</u>, <strong>Wei Zeng*</strong><BR>
                    <em>Proceedings of The ACM CHI Conference on Human Factors in Computing Systems</em>, 2024, Article 182, 1–18.<BR>
                    [<a href="https://arxiv.org/abs/2401.15559">arxiv</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_typedance.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>TypeDance: Creating Semantic Typographic Logos from Image through Personalized Generation</b><BR>
                    <u>Shishi Xiao</u>, <u>Liangwei Wang</u>, Xiaojuan Ma, <strong>Wei Zeng*</strong><BR>
                    <em>Proceedings of The ACM CHI Conference on Human Factors in Computing Systems</em>, 2024, Article 175, 1–18.<BR>
                    [<a href="https://arxiv.org/abs/2401.11094">arxiv</a>]
                </td>
            </tr>
            </tbody>


            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_plantography.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>PlantoGraphy: Incoporating Iterative Design Process into Generative Artificial Intelligence for Landscape Rendering</b><BR>
                    <u>Rong Huang</u>, <u>Haichuan Lin</u>, <u>Chuanzhang Chen</u>, Kang Zhang, <strong>Wei Zeng*</strong><BR>
                    <em>Proceedings of The ACM CHI Conference on Human Factors in Computing Systems</em>, 2024, Article 168, 1–19.<BR>
                    [<a href="https://arxiv.org/abs/2401.17120">arxiv</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_situated_interaction.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>“Make Interaction Situated”: Designing User Acceptable Interaction for Situated Visualization in Public Environments</b><BR>
                    <u>Qian Zhu</u>, Zhuo Wang, <strong>Wei Zeng*</strong>, Wai Tong, Weiyue Lin, Xiaojuan Ma<BR>
                    <em>Proceedings of The ACM CHI Conference on Human Factors in Computing Systems</em>, 2024, Article 196, 1–21.<BR>
                    [<a href="https://arxiv.org/abs/2402.14251">arxiv</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_vr_stroke.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Generating Virtual Reality Stroke Gesture Data from Out-of-Distribution Desktop Stroke Gesture Data</b><BR>
                    <u>Linping Yuan</u>, Boyu Li, Jindong Wang, Huamin Qu, <strong>Wei Zeng*</strong><BR>
                    <em>Proceedings of The IEEE Conference on Virtual Reality and 3D User Interfaces</em>, 2024, pp. 732-742.<BR>
                    [<a href="https://doi.org/10.1109/VR58804.2024.00093">DOI</a>]
                    [<a href="https://github.com/yuanlinping/VRStrokeOOD">Code</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_cscw_contemporary%20art.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>The Contemporary Art of Image Search: Iterative User Intent Expansion via Vision-Language Model</b><BR>
                    <u>Yilin Ye</u>, <u>Qian Zhu</u>, <u>Shishi Xiao</u>, Kang Zhang, <strong>Wei Zeng*</strong><BR>
                    <em>Proc. ACM Hum.-Comput. Interact.</em>, vol. 8, no. CSCW1, Article 180:1-31, 2024.<BR>
                    [<a href="https://arxiv.org/abs/2312.01656">arxiv</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2024_MetroBUX.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>MetroBUX: A Topology-based Visual Analytics for Bus Operational Uncertainty EXploration</b><BR>
                    <u>Shishi Xiao</u>, <u>Qing Shi</u>, <u>Lingdan Shao</u>, Bo Du, Yang Wang, Qiaomu Shen, <strong>Wei Zeng*</strong><BR>
                    <em>IEEE Transactions on Intelligent Transportation Systems</em>, vol. 25, no. 6, pp. 5525-5538, 2024.<BR>
                    [<a href="https://doi.org/10.1109/TITS.2023.3338700">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2023-chartspark.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Let the Chart Spark: Embedding Semantic Context into Chart with Text-to-Image Generative Model</b><BR>
                    <u>Shishi Xiao</u>, Suizi Huang, Yue Lin, <u>Yilin Ye</u>, <strong>Wei Zeng*</strong><BR>
                    <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE VIS 2023)</em>, 30(1): 284 - 294, 2024.<BR>
                    [<a href="https://arxiv.org/abs/2304.14630">arxiv</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2023-timetuner.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations</b><BR>
                    <u>Jianing Hao</u>, <u>Qing Shi</u>, <u>Yilin Ye</u>, <strong>Wei Zeng*</strong><BR>
                    <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE VIS 2023)</em>, 30(1): 1183 - 1193, 2024. <BR>
                    [<a href="https://arxiv.org/abs/2307.09916">arxiv</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2023-wytywyr.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>WYTIWYR: A User Intent-Aware Framework with Multi-modal Inputs for Visualization Retrieval</b><BR>
                    <u>Shishi Xiao</u>, <u>Yihan Hou</u>, Cheng Jin, <strong>Wei Zeng*</strong><BR>
                    <em>Computer Graphics Forum (Proc. EuroVis 2023)</em>, 42(3): 311-322, 2023. <BR>
                    [<a href="https://arxiv.org/abs/2304.06991">arxiv</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2023%20layout_adaption.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Semi-Automatic Layout Adaptation for Responsive Multiple-View Visualization Design</b><BR>
                    <strong>Wei Zeng*</strong>, <u>Xi Chen</u>, <u>Yihan Hou</u>, <u>Lingdan Shao</u>, <u>Zhe Chu</u>, Remco Chang <BR>
                    <em>IEEE Transactions on Visualization and Computer Graphics</em>, vol. 30, no. 7, pp. 3798 - 3812, 2024.<BR>
                    [<a href="https://hkust-cival.com/projects/adaMV/">Homepage</a>]
                    [<a href="https://hkust-cival.com/projects/adaMV/material/0_adapative_layout.pdf">Paper</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2022_visatlas.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>VISAtlas: An Image-based Exploration and Query System for Large Visualization Collections via Neural Image Embedding</b><BR>
                    <u>Yilin Ye</u>, <u>Rong Huang</u>, <strong>Wei Zeng*</strong><BR>
                    <em>IEEE Transactions on Visualization and Computer Graphics</em>, vol. 30, no. 7, pp. 3224 - 3240, 2024.<BR>
                    [<a href="https://hkust-cival.com/projects/visatlas/">Homepage</a>]
                    [<a href="https://hkust-cival.com/projects/visatlas/material/vis_atlas.pdf">Paper</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2022_mv_ar.jpg">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Effects of View Layout on Situated Analytics for Multiple Representations in Immersive Visualization</b><BR>
                    <u>Zhen Wen</u>, <strong>Wei Zeng*</strong>, Luoxuan Weng, Yihan Liu, Mingliang Xu, Wei Chen*<BR>
                    <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE VIS 2022)</em>, 29(1): 440 - 450, 2023.<BR>
                    [<a href="https://hkustgz-my.sharepoint.com/:b:/g/personal/weizeng_hkust-gz_edu_cn/EVwKqTKNaPVGkkP5qunovLkBV85lLlAQhH9284l4-cfxzQ?e=PhTyfg">Paper</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2021_actfloor-gan.jpg">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>ActFloor-GAN: Activity-Guided Adversarial Networks for Human-Centric Floorplan Design</b><BR>
                    <u>Shidong Wang</u>, <strong>Wei Zeng*</strong>, <u>Xi Chen</u>, Yu Ye, Yu Qiao, Chi-Wing Fu<BR>
                    <em>IEEE Transactions on Visualization and Computer Graphics</em>, 29(3): 1610-1624, 2023.<BR>
                    [<a href="https://arxiv.org/abs/2111.03545">Paper</a>]
<!--                    [<a href="https://doi.org/10.1109/TKDE.2021.3112977">DOI</a>]-->
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2021_convolutions.jpg">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Modeling Spatial Nonstationarity via Deformable Convolutions for Deep Traffic Flow Prediction</b><BR>
                    <strong>Wei Zeng</strong>, <u>Chengqiao Lin</u>, Kang Liu, Juncong Lin*, Anthony K. H. Tung<BR>
                    <em>IEEE Transactions on Knowledge and Data Engineering</em>, 35(3): 2796 - 2808, 2023.<BR>
                    [<a href="https://arxiv.org/abs/2101.12010">Paper</a>]
                    [<a href="https://doi.org/10.1109/TKDE.2021.3112977">DOI</a>]
                </td>
            </tr>
            </tbody>

<!--            <tbody style="height: 110px"><tr>-->
<!--                <td>-->
<!--                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2021_urbanvr.png">-->
<!--                </td>-->
<!--                <td style="padding-left: 10px; vertical-align: top">-->
<!--                    <b>UrbanVR: An immersive analytics system for context-aware urban design</b><BR>-->
<!--                    <u>Chi Zhang</u>, <strong>Wei Zeng*</strong>, Ligang Liu<BR>-->
<!--                    <em>Computers & Graphics</em>, vol. 99, pp. 128-138, 2021. <BR>-->
<!--                    [<a href="https://arxiv.org/abs/2107.00227">Paper</a>]-->
<!--                    [<a href="https://youtu.be/GIq8U7o9_fQ">Video</a>]-->
<!--                    [<a href="https://doi.org/10.1016/j.cag.2021.07.006">DOI</a>]-->
<!--                </td>-->
<!--            </tr>-->
<!--            </tbody>-->

<!--            <tbody style="height: 110px"><tr>-->
<!--                <td>-->
<!--                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2021_bayesmvlayout.png">-->
<!--                </td>-->
<!--                <td style="padding-left: 10px; vertical-align: top">-->
<!--                    <b>Modeling Layout Design for Multiple-View Visualization via Bayesian Inference</b><BR>-->
<!--                    <u>Lingdan Shao</u>, <u>Zhe Chu</u>, <u>Xi Chen</u>, <u>Yanna Lin</u>, <strong>Wei Zeng*</strong><BR>-->
<!--                    <em>Journal of Visualization (Proc. ChinaVis 2021 <span style="color: red">Best Paper Honorable Mention</span>)</em>,  <BR>-->
<!--                    vol. 24, no. 6, pp. 1237-1252, 2021. <br>-->
<!--                    [<a href="https://lingdan33.github.io/bayesmvlayout/">Homepage</a>]-->
<!--                    [<a href="papers/2021%20bayes_mv_layout.pdf">Paper</a>]-->
<!--                    [<a href="https://doi.org/10.1007/s12650-021-00781-z">DOI</a>]-->
<!--                </td>-->
<!--            </tr>-->
<!--            </tbody>-->

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2021_colormap.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Deep Colormap Extraction from Visualizations</b><BR>
                    <u>Lin-Ping Yuan</u>, <strong>Wei Zeng*</strong>, Siwei Fu, Zhiliang Zeng, Haotian Li, Chi-Wing Fu, Huamin Qu<BR>
                    <em>IEEE Transactions on Visualization and Computer Graphics</em>, 28(12): 4048 - 4060, 2022.<BR>
                    [<a href="https://github.com/yuanlinping/deep_colormap_extraction">Code & Data</a>]
                    [<a href="https://arxiv.org/abs/2103.00741">Paper</a>]
                    [<a href="https://doi.org/10.1109/TVCG.2021.3070876">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="90" style="border: 2px lightgrey solid;" src="./screenshot/2021_floorlevel.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>FloorLevel-Net: Recognizing Floor-Level Lines with Height-Attention-Guided Multi-task Learning</b><BR>
                    <u>Mengyang Wu</u>, <strong>Wei Zeng*</strong>, Chi-Wing Fu*<BR>
                    <em>IEEE Transactions on Image Processing</em>, 30: 6686-6699, 2021.<BR>
                    [<a href="https://wumengyangok.github.io/Project/FloorLevelNet">Code & Data</a>]
                    [<a href="https://arxiv.org/abs/2107.02462">Paper</a>]
                    [<a href="https://doi.org/10.1109/TIP.2021.3096090">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="100" style="border: 2px lightgrey solid;" src="./screenshot/2020_mv_landscape.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Composition and Configuration Patterns in Multiple-View Visualizations</b><BR>
                    <u>Xi Chen</u>, <strong>Wei Zeng*</strong>, <u>Yanna Lin</u>, H. M. Al-maneea, J. C. Roberts, R. Chang<BR>
                    <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE InfoVis 2020)</em>,<br> 27(2): 1514-1524, 2021.<BR>
                    [<a href="https://mvlandscape.bitbucket.io/">Homepage</a>]
                    [<a href="https://arxiv.org/abs/2007.15407">Paper</a>]
                    [<a href="https://youtu.be/WSPCbIdDnFQ">Video</a>]
                    [<a href="https://valt.cs.tufts.edu/papers/multiple-view/mv_data.zip">Data</a>]
                    [<a href="https://doi.org/10.1109/TVCG.2020.3030338">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="100" style="border: 2px lightgrey solid;" src="./screenshot/2020_vis4maup.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics</b><BR>
                    <strong>Wei Zeng</strong>, <u>Chengqiao Lin</u>, Juncong Lin*, Jincheng Jiang, Jiazhi Xia, C. Turkay, Wei Chen<BR>
                    <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE VAST 2020)</em>,<br> 27(2): 839-848, 2021.<BR>
                    [<a href="https://arxiv.org/abs/2007.15486">Paper</a>]
                    [<a href="https://youtu.be/FC7dRXbrZ4s">Video</a>]
                    [<a href="https://doi.org/10.1109/TVCG.2020.3030410">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="100" style="border: 2px lightgrey solid;" src="./screenshot/2020_topology_density_map.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Topology Density Map for Urban Data Visualization and Analysis</b><BR>
                    <u>Zezheng Feng</u>, <u>Haotian Li</u>, <strong>Wei Zeng*</strong>, Shuang-Hua Yang, Huamin Qu<BR>
                    <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE VAST 2020)</em>,<br> 27(2): 828-838, 2021.<BR>
                    [<a href="https://arxiv.org/abs/2007.15828">Paper</a>]
                    [<a href="https://doi.org/10.1109/TVCG.2020.3030469">DOI</a>]
                </td>
            </tr>
            </tbody>

<!--            <tbody style="height: 110px"><tr>-->
<!--                <td>-->
<!--                    <img width="150" height="100" style="border: 2px lightgrey solid;" src="./screenshot/2020_examplar_graph_layout.png">-->
<!--                </td>-->
<!--                <td style="padding-left: 10px; vertical-align: top">-->
<!--                    <b>Exemplar-based Layout Fine-tuning for Node-link Diagrams</b><BR>-->
<!--                    Jiacheng Pan, Wei Chen, Xiaodong Zhao, Shuyue Zhou, <strong>Wei Zeng</strong>, Minfeng Zhu, Jian Chen, Siwei Fu, Yingcai Wu<BR>-->
<!--                    <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE InfoVis 2020)</em>,<br> 27(2): 1655-1665, 2021.<BR>-->
<!--                    [<a href="https://arxiv.org/abs/2008.00666">Paper</a>]-->
<!--                    [<a href="https://doi.org/10.1109/TVCG.2020.3030393">DOI</a>]-->
<!--                </td>-->
<!--            </tr>-->
<!--            </tbody>-->

<!--            <tbody style="height: 110px"><tr>-->
<!--                <td>-->
<!--                    <img width="150" height="100" style="border: 2px lightgrey solid;" src="./screenshot/2019_vis_fig.png">-->
<!--                </td>-->
<!--                <td style="padding-left: 10px; vertical-align: top">-->
<!--                    <b>VIStory: Interactive Storyboard for Exploring Visual Information in Scientific Publications</b><BR>-->
<!--                    <strong>Wei Zeng</strong>, <u>Ao Dong</u>, <u>Xi Chen</u>, Zhanglin Cheng<br>-->
<!--                    <em>Journal of Visualization (Proc. VINCI 2019 <span style="color: red">Best Paper</span>)</em>,-->
<!--                    vol. 24, no. 1, pp. 69-84, 2021.<BR>-->
<!--                    [<a href="https://dongoa.github.io/projects/VIStory/index.html">Homepage</a>]-->
<!--                    [<a href="http://link.springer.com/article/10.1007/s12650-020-00688-1">Paper</a>]-->
<!--                    [<a href="https://dongoa.github.io/VIStory/">Interface</a>]-->
<!--                    [<a href="https://youtu.be/lSCuySOEoM8">Youtube</a>]-->
<!--                    [<a href="https://doi.org/10.1007/s12650-020-00688-1">DOI</a>]-->
<!--                </td>-->
<!--            </tr>-->
<!--            </tbody>-->

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="100" style="border: 2px lightgrey solid;" src="./screenshot/2020_overlay.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Deep Recognition of Vanishing-Point-Constrained Building Planes in Urban Street Views</b><BR>
                    <u>Zhiliang Zeng</u>, <u>Mengyang Wu</u>, <strong>Wei Zeng*</strong>, and Chi-Wing Fu<BR>
                    <em>IEEE Transactions on Image Processing</em>, 29: 5912-5923, 2020.<BR>
                    [<a href="papers/2020%20ARScape.pdf">Paper</a>]
                    [<a href="https://doi.org/10.1109/TIP.2020.2986894">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="100" style="border: 2px lightgrey solid;" src="./screenshot/2019_lassonet.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>LassoNet: Deep Lasso-Selection of 3D Point Clouds </b><BR>
                    <u>Zhutian Chen</u>, <strong>Wei Zeng*</strong>, <u>Zhiguang Yang</u>, Lingyun Yu, Chi-Wing Fu, and Huamin Qu<BR>
                    <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE SciVis 2019)</em><BR>
                    26(1): 195-204, 2020.<BR>
                    [<a href="https://arxiv.org/pdf/1907.13538.pdf">Paper</a>]
                    [<a href="https://lassonet.github.io/">Homepage</a>]
                    [<a href="https://youtu.be/OsE_LmR-Ec4">Video</a>]
                    [<a href="https://doi.org/10.1109/TVCG.2019.2934332">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="100" style="border: 2px lightgrey solid;" src="./screenshot/2019_shenzhen_raeb.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Route-Aware Edge Bundling for Visualizing Origin-Destination Trails in Urban Traffic</b><BR>
                    <strong>Wei Zeng</strong>, <u>Qiaomu Shen</u>, <u>Yuzhe Jiang</u>, Alex Telea<BR>
                    <em>Computer Graphics Forum (Proc. EuroVis 2019)</em>,
                    38(3): 581-593, 2019.<BR>
                    [<a href="papers/2019_RAEB.pdf">Paper</a>]
                    [<a href="../presentation/EuroVis19_ppt.pdf">Presentation</a>]
                    [<a href="https://youtu.be/SHZmj4p1WwQ">Video</a>]
                    [<a href="https://doi.org/10.1111/cgf.13712">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="100" style="border: 2px lightgrey solid;" src="./screenshot/2018-vitalvizor.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>VitalVizor: A Visual Analytics System for Studying Urban Vitality</b><BR>
                    <strong>Wei Zeng*</strong>, Yu Ye<br>
                    <em>IEEE Computer Graphics and Applications (Special Issue on Visualization for Smart City Application)</em>,
                    38(5): 38-53, 2018.<br>
                    [<a href="papers/2018%20VitalVizor.pdf">Paper</a>]
                    [<a href="https://www.youtube.com/watch?v=MAxNu6jc2mo">Video</a>]
                    [<a href="https://doi.org/10.1109/MCG.2018.053491730">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="100" style="border: 2px lightgrey solid;" src="./screenshot/2018_streetvizor.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>StreetVizor: Visual Exploration of Human-Scale Urban Forms Based on Street Views</b><BR>
                    <u>Qiaomu Shen</u>, <strong>Wei Zeng*</strong>, Yu Ye, Stefan Müller Arisona, Simon Schubiger, Remo Burkhard and Huamin Qu<br>
                    <em>IEEE Transactions on Visualization and Computer Graphics (Proceedings of IEEE SciVis'17)</em>
                    24(1): 1004 - 1013, 2018.<br>
                    [<a href="papers/2018%20streetvizor.pdf">Paper</a>]
                    [<a href="https://www.youtube.com/watch?v=K-bCY71P8MM&list=PL3-tIMzrYMbKRmrau4n1HIbMJMzSma8wh&index=8&t=47s">Video</a>]
                    [<a href="https://doi.org/10.1109/TVCG.2017.2744159">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="100" style="border: 2px lightgrey solid;" src="./screenshot/2017_poi_sign.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Visualizing the Relationship between Human Mobility and Points-of-Interest</b><BR>
                    <strong>Wei Zeng</strong>, Chi-Wing Fu, Stefan Müller Arisona, Simon Schubiger, Remo Burkhard and Kwan-Liu Ma<br>
                    <em>IEEE Transactions on Intelligent Transportation Systems (Special Issue on Visual Analysis for ITS)</em>
                    18(8): 2271-2284, 2017.<br>
                    [<a href="papers/2017%20POI.pdf">Paper</a>]
                    [<a href="https://youtu.be/aaBU9xJpOj0">Video</a>]
                    [<a href="https://doi.org/10.1109/TITS.2016.2639320">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="100" style="border: 2px lightgrey solid;" src="./screenshot/2016_od.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Visualizing Waypoints-Constrained Origin-Destination Patterns for Massive Transportation Data</b><BR>
                    <strong>Wei Zeng</strong>, Chi-Wing Fu, Stefan Müller Arisona, Alexander Erath, Huamin Qu<br>
                    <em>Computer Graphics Forum (Proceedings of EuroVis'16)</em>,
                    35(8): 95-107, 2016.<br>
                    [<a href="papers/2016%20Waypoints%20OD.pdf" rel="noopener">Paper</a>]
                    [<a href="https://www.youtube.com/watch?v=lAdqxSURCGQ">Video</a>]
                    [<a href="https://doi.org/10.1111/cgf.12778">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="100" style="border: 2px lightgrey solid;" src="./screenshot/2014_vis_mobility.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Visualizing Mobility of Public Transportation System</b><BR>
                    <strong>Wei Zeng</strong>, Chi-Wing Fu, Stefan Müller Arisona, Alexander Erath, Huamin Qu<br>
                    <em>IEEE Transactions on Visualization and Computer Graphics (Proceedings of IEEE VAST'14)</em>,
                    20(12): 1833-1842, 2014.<br>
                    [<a href="papers/2014%20Mobility.pdf" rel="noopener">Paper</a>]
                    [<a href="http://youtube=http://www.youtube.com/watch?v=Q95c8PpneT8">Video</a>]
                    [<a href="https://doi.org/10.1109/TVCG.2014.2346893">DOI</a>]
                </td>
            </tr>
            </tbody>

            <tbody style="height: 110px"><tr>
                <td>
                    <img width="150" height="100" style="border: 2px lightgrey solid;" src="./screenshot/2013_interchange.png">
                </td>
                <td style="padding-left: 10px; vertical-align: top">
                    <b>Visualizing Interchange Patterns in Massive Movement Data</b><BR>
                    <strong>Wei Zeng</strong>, Chi-Wing Fu, Stefan Müller Arisona and Huamin Qu<br>
                    <em>Computer Graphics Forum (Proceedings of EuroVis'13)</em>,
                    32(3pt3): 271-280, 2013.<br>
                    [<a href="papers/2013%20interchange.pdf" rel="noopener">Paper</a>]
                    [<a href="http://www.youtube.com/watch?v=_QWnA1k2ZrU" rel="noopener">Video</a>]
                    [<a href="https://doi.org/10.1111/cgf.12114">DOI</a>]
                </td>
            </tr>
            </tbody>
        </table>

    </div>
</div>

</body>
</html>
