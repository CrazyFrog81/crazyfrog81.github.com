<!doctype html>
<html xmlns="http://www.w3.org/1999/html">
<head>
    <meta name="Robots" contect= "none">
    <meta charset="UTF-8">

    <title>Zeng Wei's Homepage</title>
    <link rel="stylesheet" type="text/css" href="zengwei.css" media="all" />
    <script src="info.js" type="text/javascript" defer></script>
</head>

<body>

<div class="content">
    <div id="info" class="id">
    </div>

    <div class="main">
        <nav id="nav">
            <a style="font-weight:bold">HOME</a> &nbsp;&nbsp;
            <a href="Research">RESEARCH</a> &nbsp;&nbsp;
            <a href="publication">PUBLICATIONS</a> &nbsp;&nbsp;
            <a href="http://www.hkust-cival.com">TEAM</a> &nbsp;&nbsp;
            <a href="vita">VITA</a> &nbsp;&nbsp;
<!--            <a href="blog">BLOG</a> &nbsp;&nbsp;-->
            <a href="bookmark">BOOKMARK</a> &nbsp;&nbsp;
        </nav>

        <div>
            I'm an assistant professor at <a href="https://cma.hkust-gz.edu.cn/">Thrust of Computational Media and Arts (CMA)</a>, and jointly appointed at <a href="https://hkust-gz.edu.cn/academics/four-hubs/information-hub/data-science-and-analytics">Thrust of Data Science and Analytics (DSA)</a>
            in <a href="https://hkust-gz.edu.cn/academics/four-hubs/information-hub">Information Hub</a>, <a href="https://hkust-gz.edu.cn/">the Hong Kong University of Science and Technology (Guangzhou)</a>, and also affiliated with Department of Computer Science and Engineering, HKUST.
            My research focuses on data visualization and visual analytics, VR/AR, and creative design.
            Before joining HKUST (GZ), I worked as an associate professor at <a href="https://siat.cas.cn/">Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences</a>,
            adjunct associate professor at <a href="https://uic.edu.cn/en/"> BNU-HKBU United International College </a>,
            and a senior researcher at <a href="https://fcl.ethz.ch/">Future Cities Laboratory</a>, <a href="https://ethz.ch/en.html">ETH Zurich</a>.
            I received both the bachelor and Ph.D. degrees in computer sciences from the <a href="https://www.ntu.edu.sg/Pages/home.aspx">Nanyang Technological University</a>.<br/>
            <br>
         I'm leading the Collaborative Interactive Visualization & Analysis Laboratory (CIVAL) @ HKUST (GZ). Find more information about CIVAL <a href="http://www.hkust-cival.com">here</a>.
        </div>
            <h4 style="color: red">Now looking for!!</h4>
            <ul>
                <LI>
                    I'm actively looking for PostDocs, PhDs, masters, and RAs.
                    Drop an email with your resume and research plan if you are <B>hardworking, creative, and well-motivated for high-quality research!</B>
                </li>

                <li style="margin-top: 7px">
                    <em>Prospective students</em>: Please take a look at my research profile and interests, and write a concrete research plan accordingly.
                    I will not respond to your email if the content is generic in any research area, for example, "I would like to do research in data visualization" or "in deep learning."
                    I do have a wide spectrum of research interests, but that does not mean that I am dedicated to or capable of doing any research.
                    It is fine if you know nothing about my research, in case that you are willing to and be able to do the research assigned to you!
                </li>

<!--                <li style="margin-top: 7px">-->
<!--                    Application for HKUST(GZ) Pilot Scheme 2022/23 has opened. Find more info-->
<!--                    <a href="https://pg.usthk.cn/prospective-students/admissions/HKUST-Guangzhou-Pilot-Scheme/GZ-Pilot-Scheme">here</a>.-->
<!--                </li>-->
            </ul>

        <h4 style="margin:25px 0 5px 0">Recent News</h4>
        <ul>
            <li style="margin-bottom: 7px">
                July '24: Two papers got accepted to IEEE VIS 2024.
                <ul>
                    <li>'<em>ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via Modal Fusion Map</em>'. Congrats to Yilin Ye.</li>
                    <li>'<em>Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning</em>'. Congrats to Xingchen Zeng.</li>
                </ul>
            </li>

            <li style="margin-bottom: 7px">
                July '24: Our paper '<em>HarmonyWave: Immersive Space Sound Therapy with Audio-Visual Synesthesia</em>'
                got accepted to VINCI'24 Art Paper track. Congrats to Jian Yu.
            </li>

            <li style="margin-bottom: 7px">
                May '24: Our paper '<em>Antarctica Storytelling: Creating Interactive Story Maps for Polar Regions with Graphic-Based Approach</em>'
                got accepted to The Visual Computer. Congrats to Liangwei Wang.
            </li>

            <li style="margin-bottom: 7px">
                May '24: Our paper '<em>NFTracer: Tracing NFT Impact Dynamics in Transaction-flow Substitutive Systems with Visual Analytics</em>'
                got accepted to IEEE TVCG. Congrats to Yifan Cao.
            </li>

            <li style="margin-bottom: 7px">
                May '24: Our paper '<em>GeoReasoner: Geo-localization with Reasoning in Street Views using a Large Vision-Language Model</em>'
                got accepted to ICML'24.
                Congrats to Ling Li.
            </li>

<!--            <li style="margin-bottom: 7px">-->
<!--                Apr. '24: Our survey paper '<em>Generative AI for Visualization: State of the Art and Future Directions</em>'-->
<!--                got accepted to Visual Informatics.-->
<!--                Congrats to Yilin Ye.-->
<!--            </li>-->

<!--            <li style="margin-bottom: 7px">-->
<!--                Mar. '24: Our paper '<em>CheetahTraj: Efficient Visualization for Large Trajectory Dataset with Quality Guarantee</em>'-->
<!--                got accepted to IEEE Transactions on Knowledge and Data Engineering.-->
<!--                Congrats to Dr. Qiaomu Shen.-->
<!--            </li>-->

<!--            <li style="margin-bottom: 7px">-->
<!--                Mar. '24: Our paper '<em>Understanding the Impact of Referent Design on Scale Perception in Immersive Data Visualization</em>'-->
<!--                got accepted to ACM CHI Late-Breaking 2024.-->
<!--                Congrats to Yihan Hou.-->
<!--            </li>-->

<!--            <li style="margin-bottom: 7px">-->
<!--                Jan '24: Six papers got accepted to ACM CHI 2024.-->
<!--                <ul>-->
<!--                    <li>'<em>VirtuWander: Enhancing Multi-modal Interaction for Virtual Tour Guidance through Large Language Models</em>'. Congrats to Zhan Wang.</li>-->
<!--                    <li>'<em>C2ldeas: Supporting Creative Interior Color Design Ideation with Large Language Model</em>'. Congrats to Yihan Hou.</li>-->
<!--                    <li>'<em>IntentTuner: An Interactive Framework for Integrating Human Intentions in Fine-tuning Text-to-Image Generative Models</em>'. Congrats to Xingchen Zeng.</li>-->
<!--                    <li>'<em>TypeDance: Creating Semantic Typographic Logos from Image through Personalized Generation</em>'. Congrats to Shishi Xiao.</li>-->
<!--                    <li>'<em>PlantoGraphy: Incoporating Iterative Design Process into Generative Artificial Intelligence for Landscape Rendering</em>'. Congrats to Rong Huang.</li>-->
<!--                    <li>'<em>“Make Interaction Situated”: Designing User Acceptable Interaction for Situated Visualization in Public Environments</em>'. Congrats to Qian Zhu.</li>-->
<!--                </ul>-->
<!--            </li>-->
        </ul>
    </div>

</div>

</body>
</html>