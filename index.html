<!doctype html>
<html xmlns="http://www.w3.org/1999/html">
<head>
    <meta name="Robots" contect= "none">
    <meta charset="UTF-8">

    <title>Zeng Wei's Homepage</title>
    <link rel="stylesheet" type="text/css" href="zengwei.css" media="all" />
    <script src="info.js" type="text/javascript" defer></script>
</head>

<body>

<div class="content">
    <div id="info" class="id">
    </div>

    <div class="main">
        <nav id="nav">
            <a style="font-weight:bold">HOME</a> &nbsp;&nbsp;
            <a href="Research">RESEARCH</a> &nbsp;&nbsp;
            <a href="publication">PUBLICATIONS</a> &nbsp;&nbsp;
            <a href="http://www.hkust-cival.com">TEAM</a> &nbsp;&nbsp;
            <a href="vita">VITA</a> &nbsp;&nbsp;
<!--            <a href="blog">BLOG</a> &nbsp;&nbsp;-->
            <a href="bookmark">BOOKMARK</a> &nbsp;&nbsp;
        </nav>

        <div>
            I'm an assistant professor at <a href="https://cma.hkust-gz.edu.cn/">Thrust of Computational Media and Arts (CMA)</a>, and jointly appointed at <a href="https://hkust-gz.edu.cn/academics/four-hubs/information-hub/data-science-and-analytics">Thrust of Data Science and Analytics (DSA)</a>
            in <a href="https://hkust-gz.edu.cn/academics/four-hubs/information-hub">Information Hub</a>, <a href="https://hkust-gz.edu.cn/">the Hong Kong University of Science and Technology (Guangzhou)</a>, and also affiliated with Department of Computer Science and Engineering, HKUST.
            My research focuses on data visualization and visual analytics, VR/AR, and creative design.
            Before joining HKUST (GZ), I worked as an associate professor at <a href="https://siat.cas.cn/">Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences</a>,
            adjunct associate professor at <a href="https://uic.edu.cn/en/"> BNU-HKBU United International College </a>,
            and a senior researcher at <a href="https://fcl.ethz.ch/">Future Cities Laboratory</a>, <a href="https://ethz.ch/en.html">ETH Zurich</a>.
            I received both the bachelor and Ph.D. degrees in computer sciences from the <a href="https://www.ntu.edu.sg/Pages/home.aspx">Nanyang Technological University</a>.<br/>
            <br>
         I'm leading the Collaborative Interactive Visualization & Analysis Laboratory (CIVAL) @ HKUST (GZ). Find more information about CIVAL <a href="http://www.hkust-cival.com">here</a>.
        </div>
            <h4 style="color: red">Now looking for!!</h4>
            <ul>
                <LI>
                    I'm actively looking for PostDocs, PhDs, masters, and RAs.
                    Drop an email with your resume and research plan if you are <B>hardworking, creative, and well-motivated for high-quality research!</B>
                </li>

                <li style="margin-top: 7px">
                    <em>Prospective students</em>: Please take a look at my research profile and interests, and write a concrete research plan accordingly.
                    I will not respond to your email if the content is generic in any research area, for example, "I would like to do research in data visualization" or "in deep learning."
                    I do have a wide spectrum of research interests, but that does not mean that I am dedicated to or capable of doing any research.
                    It is fine if you know nothing about my research, in case that you are willing to and be able to do the research assigned to you!
                </li>

<!--                <li style="margin-top: 7px">-->
<!--                    Application for HKUST(GZ) Pilot Scheme 2022/23 has opened. Find more info-->
<!--                    <a href="https://pg.usthk.cn/prospective-students/admissions/HKUST-Guangzhou-Pilot-Scheme/GZ-Pilot-Scheme">here</a>.-->
<!--                </li>-->
            </ul>

        <h4 style="margin:25px 0 5px 0">Recent News</h4>
        <ul>
            <li style="margin-bottom: 7px">
                Apr. '24: Our survey paper '<em>Generative AI for Visualization: State of the Art and Future Directions</em>'
                got accepted to Visual Informatics.
                Congrats to Yilin Ye.
            </li>

            <li style="margin-bottom: 7px">
                Mar. '24: Our paper '<em>CheetahTraj: Efficient Visualization for Large Trajectory Dataset with Quality Guarantee</em>'
                got accepted to IEEE Transactions on Knowledge and Data Engineering.
                Congrats to Dr. Qiaomu Shen.
            </li>

            <li style="margin-bottom: 7px">
                Mar. '24: Our paper '<em>Understanding the Impact of Referent Design on Scale Perception in Immersive Data Visualization</em>'
                got accepted to ACM CHI Late-Breaking 2024.
                Congrats to Yihan Hou.
            </li>

            <li style="margin-bottom: 7px">
                Jan '24: Six papers got accepted to ACM CHI 2024.
                <ul>
                    <li>'<em>VirtuWander: Enhancing Multi-modal Interaction for Virtual Tour Guidance through Large Language Models</em>'. Congrats to Zhan Wang.</li>
                    <li>'<em>C2ldeas: Supporting Creative Interior Color Design Ideation with Large Language Model</em>'. Congrats to Yihan Hou.</li>
                    <li>'<em>IntentTuner: An Interactive Framework for Integrating Human Intentions in Fine-tuning Text-to-Image Generative Models</em>'. Congrats to Xingchen Zeng.</li>
                    <li>'<em>TypeDance: Creating Semantic Typographic Logos from Image through Personalized Generation</em>'. Congrats to Shishi Xiao.</li>
                    <li>'<em>PlantoGraphy: Incoporating Iterative Design Process into Generative Artificial Intelligence for Landscape Rendering</em>'. Congrats to Rong Huang.</li>
                    <li>'<em>“Make Interaction Situated”: Designing User Acceptable Interaction for Situated Visualization in Public Environments</em>'. Congrats to Qian Zhu.</li>
                </ul>
            </li>
<!--            <li style="margin-bottom: 7px">-->
<!--                Jan. '24: Our paper '<em>Generating Virtual Reality Stroke Gesture Data from Out-of-Distribution Desktop Stroke Gesture Data</em>'-->
<!--                got accepted to IEEE VR 2024.-->
<!--                Congrats to Linping Yuan.-->
<!--            </li>-->

<!--            <li style="margin-bottom: 7px">-->
<!--                Nov. '23: Our paper '<em>The Contemporary Art of Image Search: Iterative User Intent Expansion via Vision-Language Model</em>'-->
<!--                got accepted to CSCW 2024.-->
<!--                Congrats to Yilin Ye.-->
<!--            </li>-->

<!--            <li style="margin-bottom: 7px">-->
<!--                Nov. '23: Our paper '<em>MetroBUX: A Topology-based Visual Analytics for Bus Operational Uncertainty EXploration</em>'-->
<!--                got accepted to IEEE Transactions on Intelligent Transportation Systems.-->
<!--                Congrats to Shishi Xiao, Qing Shi, and Lingdan Shao.-->
<!--            </li>-->

<!--            <li style="margin-bottom: 7px">-->
<!--                Nov. '23: Our paper '<em>HoLens: A Visual Analytics Design for Higher-order Movement Modeling and Visualization</em>' got accepted to Computational Visual Media.-->
<!--                Congrats to Zezheng Feng.-->
<!--            </li>-->
<!--            <li style="margin-bottom: 7px">-->
<!--                July '23: Our paper '<em>Storytelling in Frozen Frontier: Exploring Graphic-Based Approach for Creating Interactive Story Maps in Antarctica</em>' received Best Paper Award in VINCI 2023.-->
<!--                Congrats to Liangwei Wang.-->
<!--            </li>-->
<!--            <li style="margin-bottom: 7px">-->
<!--                July '23: Two papers got accepted to IEEE VIS 2023.-->
<!--                <ul>-->
<!--                    <li>'<em>TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations</em>'. Congrats to Jianing Hao.</li>-->
<!--                    <li>'<em>Let the Chart Spark: Embedding Semantic Context into Chart with Text-to-Image Generative Model</em>'. Congrats to Shishi Xiao.</li>-->
<!--                </ul>-->
<!--            </li>-->
<!--            <li style="margin-bottom: 7px">-->
<!--                July '23: Five papers (three long + two short) got accepted to VINCI 2023.-->
<!--                <ul>-->
<!--                    <li>'<em>Storytelling in Frozen Frontier: Exploring Graphic-Based Approach for Creating Interactive Story Maps in Antarctica</em>'. Congrats to Liangwei Wang.</li>-->
<!--                    <li>'<em>The Rich, the Poor, and the Ugly: An Aesthetic-Perspective Assessment of NFT Values </em>'. Congrats to Yihan Chen.</li>-->
<!--                    <li>'<em>NFTeller: Dual-centric Visual Analytics for Assessing Market Performance of NFT Collectibles </em>'. Congrats to Yifan Cao.</li>-->
<!--                    <li>'<em>Does Where You are Matter? A Visual Analytics System for COVID-19 Transmission Based on Social Hierarchical Perspective </em>'. Congrats to Jianing Hao.</li>-->
<!--                    <li>'<em>LOOP Meditation: Enhancing Novice's VR Meditation Experience with Physical Movement </em>'. Congrats to Shihan Fu.</li>-->
<!--                </ul>-->
<!--            </li>-->
<!--            <li style="margin-bottom: 7px">-->
<!--                July '23: Invited to serve as an Associate Editor for <a href="https://journalofbigdata.springeropen.com/">Journal of Big Data</a>-->
<!--                - a leading journal in data science and big data.-->
<!--            </li>-->

<!--            <li style="margin-bottom: 7px">-->
<!--                July '23: Invited to serve on the editorial board of <a href="https://www.sciencedirect.com/journal/visual-informatics">Visual Informatics</a>.-->
<!--            </li>-->
<!--            <li style="margin-bottom: 7px">-->
<!--                May. '23: I'm serving as the Program Co-Chair for <a href="http://vinci-symp.org/">VINCI'23</a>. You are welcome to submit papers!-->
<!--            </li>-->
<!--            <li style="margin-bottom: 7px">-->
<!--                Mar. '23: One paper '<em>WYTIWYR: A User Intent-Aware Framework with Multi-modal Inputs for Visualization Retrieval</em>' got accepted to EuroVis'23. Congrats to Shishi and Yihan.-->
<!--            </li>-->
<!--            <li style="margin-bottom: 7px">-->
<!--                Jan. '23: One paper '<em>Semi-Automatic Layout Adaptation for Responsive Multiple-View Visualization Design</em>' got accepted by IEEE TVCG. Congrats to Chen Xi, Yihan, Lingdan.-->
<!--            </li>-->
        </ul>
    </div>

</div>

</body>
</html>